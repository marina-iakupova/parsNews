import urllib.request
from bs4 import BeautifulSoup
import re
import string
import bs4

import codecs
def open(path, mode):
  return codecs.open(path, mode, 'utf-8')

#цикл для получения страниц новостей 1,2,3 и т.д.
mainlinks='http://lentachel.ru/sections/transport'
a=50
b=0
link=[]

while a<=1000:
    f = urllib.request.urlopen(mainlinks)
    mainlinks='http://lentachel.ru/sections/transport/%s'%a
    a=a+50
    link+=[mainlinks]
#получили массив ссылок на страницы с новостями

#на каждой странице собираем ссылки на каждую новость
b=0
s=10
while b<=19:#количество страниц с новостями
    g = urllib.request.urlopen(link[b])
    links = BeautifulSoup(g)
    b=b+1
  
    for links in links.find_all('a'):
        newlinks=links.get('href') #получили список всех ссылок на странице
        
        pattern='http://lentachel.ru/articles/\d\d\d\d\d'
        alllinks =re.findall(pattern, newlinks)#отфильтровали ссылки на нужные, занесли в переменную

        for alllinks in alllinks:
            f = urllib.request.urlopen(alllinks)
            content = BeautifulSoup(f)
            date=content.find('div', 'date')
            date=date.get_text()
            f1 = open("all.txt", 'a')
            f1.write(date)
            f1.write(' ')
            f1.write(alllinks)
            f1.write('\r\n')
            f1.close
            for content in content.find('div', 'text'):
                  textall2= content.find('p')
                  if textall2!=-1:
                      text=textall2
                      text=text.get_text()
                      f = open("D:/semestr/news/%s.txt"%s, 'a')#записываем текст всех новостей в файлики
                      s=s+1
                      f.write(date)
                      f.write('\r\n')
                      f.write(text)
                      f2= open("alltext.txt", 'a')#записываем текст всех новостей в один файл для быстрой обработки 
                      f2.write(text)
                      f2.write(' ')
                      f1.close
                      f2.close
